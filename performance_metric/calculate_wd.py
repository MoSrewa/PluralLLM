import os.path as osp
import ast
import torch
import numpy as np
from scipy.stats import wasserstein_distance
import torch.nn.functional as F
import wandb


"""
Module: alignment_wd

This module computes Wasserstein distance-based alignment scores between predicted distributions
and ground truth distributions across question groups.

It is primarily used to evaluate the quality of predicted probability distributions generated by
a model in structured group-based question datasets. The score is normalized using the maximum
possible Wasserstein distance for a given ordinal reference.
# This code is originally written in https://github.com/jamqd/Group-Preference-Optimization
"""
def calculate_WD(eval_num_qs, model, df, mode='eval', logging=True, type='fed'):
    model.eval()
    groups = df['group'].unique()
    unique_questions = df['qkey'].unique()
    context_questions = np.random.choice(unique_questions, size=eval_num_qs, replace=False)
    target_questions = np.setdiff1d(unique_questions, context_questions)
    distances_all = []

    for grp_idx, group in enumerate(groups):
        group_df = df[df['group'] == group]
        distances_group = []
        for idx, question in enumerate(target_questions):
            # Get the dataframe for the current question
            question_df = group_df[group_df['qkey'] == question]
            # Extract embeddings and probabilities
            embeddings = torch.stack([torch.tensor(e) for e in question_df['embedding'].tolist()]).unsqueeze(0).to('cuda')
            # Get the context for the current question
            context_df = group_df[group_df['qkey'].isin(context_questions)]
            context_embeddings = torch.stack([torch.tensor(e) for e in context_df['embedding'].tolist()]).unsqueeze(0).to('cuda')
            context_prob_ys = torch.tensor(context_df['prob_y'].values, dtype=torch.float).unsqueeze(0).unsqueeze(-1).to('cuda')
            if torch.isnan(context_embeddings).any():
                print("Warning: NaN values detected in context_embeddings!")
            with torch.no_grad():
                predicted_distribution_list = []  # Renamed to make it clearer that this is a list
                for i, single_embedding in enumerate(embeddings.squeeze(0)):
                    single_embedding = single_embedding.unsqueeze(0).unsqueeze(0)  # Add the batch and sequence dimensions back
                    # Generate prediction for the current embedding
                    single_predicted_distribution = model.predict(context_embeddings, context_prob_ys, single_embedding)
                    # Normalize the single predicted distribution
                    single_predicted_distribution = single_predicted_distribution.loc  # Take mean over sample dimension if needed
                    predicted_distribution_list.append(single_predicted_distribution)
                predicted_distribution = torch.stack(predicted_distribution_list)
                predicted_distribution = softmax_normalize(predicted_distribution.reshape(-1))
            
            # Convert the string representation of the list to an actual list
            D_H = ast.literal_eval(question_df['D_H'].iloc[0])

            # Convert the list to a tensor
            D_H = torch.tensor(D_H, dtype=torch.float).to('cuda')
            # Convert predicted_distribution and D_H to numpy
            predicted_distribution_np = predicted_distribution.cpu().detach().numpy()
            predicted_distribution_np = np.squeeze(predicted_distribution_np)
            
            D_H_np = np.array(D_H.cpu())
            ordinal = ast.literal_eval(question_df['ordinal'].iloc[0])
            ordinal_np = np.array(ordinal, dtype=float)
            # Compute Wasserstein distance
            if get_max_wd(ordinal_np) == 0:
                continue
            else:
                epsilon = 0
                normalized_wd = wasserstein_distance(ordinal_np, ordinal_np, predicted_distribution_np, D_H_np) / (get_max_wd(ordinal_np) + epsilon)
                distances_group.append(normalized_wd)
                distances_all.append(normalized_wd)

        mean_distance_group = np.mean(distances_group)
        if logging:
            wandb.log({f"{type.capitalize()}-{mode.capitalize()}_alignment_score_{group}": 1 - mean_distance_group})
        print(f"{mode.capitalize()}_alignment_score_{group}:  {1 - mean_distance_group}")
        
def get_max_wd(ordered_ref_weights):
    d0, d1 = np.zeros(len(ordered_ref_weights)), np.zeros(len(ordered_ref_weights))
    d0[np.argmax(ordered_ref_weights)] = 1
    d1[np.argmin(ordered_ref_weights)] = 1
    max_wd = wasserstein_distance(ordered_ref_weights, ordered_ref_weights, d0, d1)
    return max_wd

def softmax_normalize(tensor):
    """Applies softmax normalization along the last dimension of the tensor"""
    return F.softmax(tensor, dim=-1)